---
date:  February 1th, 2024
image: /blog/AI-weekly-005/banner.png
description: "Explore the latest AI Weekly updates, featuring product updates, new tutorials, and AI highlights. Dive into noteworthy papers and techniques, including Google Research's Lumiere, a comprehensive study on 26 Multimodal Large Models, and SUPIR's image restoration capabilities."
keywords: ["AI Weekly, ComfyUI, Stable Diffusion, App Logo Generation, ControlNet, Cloud Installation, ComfyUI Basic, Image-to-Image, Mobile ALOHA, IP-Adapter-FaceID, Phi-2, AI Tutorials, AI Updates, AI Technologies, AI Papers, Lumiere, Google Research, Multimodal Large Models, SUPIR, Image Restoration"]
---

# AI Weekly #005

import { PhotoProvider, PhotoView } from 'react-image-previewer';
import Subscribe from "components/subscribtion";

## üÜï What's New?

<br/>
<PhotoProvider>
  <PhotoView src="/space/001.png">
    <img src="/space/001.png" alt="" />
  </PhotoView>
</PhotoProvider>

**Product Updates:**

1. We now support the Windows version!!!!!
2. Additionally, you can now view our tutorials within the product.
3. At the same time, we've also fixed a number of bugs and improved the installation success rate.

Download: https://github.com/6174/comflowyspace/releases

**New tutorials added last week:**

- [How can ComfyUI be applied to interior design ‚ë°](https://www.comflowy.com/blog/krita-and-comfyui): Last issue we introduced how to use ComfyUI in interior design scenarios, this issue we introduce how to use Krita and ComfyUI together.

## ü§©  Weekly‚Äòs AI highlights

### üìÑ Noteworthy papers and technic

* [**Lumiere:**](https://arxiv.org/abs/2401.12945)

This is a text-to-video diffusion model developed by Google Research, employing an innovative space-time U-Net architecture. It supports a variety of video generation and editing functionalities, including text-to-video, image-to-video, stylized video generation, and video editing. The model is capable of generating an entire video in one go, ensuring both the coherence and realism of the video.

<br/>
<PhotoProvider>
  <PhotoView src="/blog/AI-weekly-005/001.png">
    <img src="/blog/AI-weekly-005/001.png" alt="" />
  </PhotoView>
</PhotoProvider>

* [**Research Report on 26 Multimodal Large Models:**](https://arxiv.org/abs/2401.13601)

This paper conducts a comprehensive study and analysis of 26 different multimodal large language models (MM-LLMs) that are currently available in the market. It provides an in-depth understanding of multimodal large language models, detailing the design of model architectures and training processes. Each of the 26 existing MM-LLMs has its unique design and functionality. The paper discusses potential future research directions and also offers resources for real-time tracking of the latest developments in these models, making it a valuable read.

<br/>
<PhotoProvider>
  <PhotoView src="/blog/AI-weekly-005/002.png">
    <img src="/blog/AI-weekly-005/002.png" alt="" />
  </PhotoView>
</PhotoProvider>

* [**SUPIR:**](https://arxiv.org/abs/2401.13627)

SUPIR enhances its image restoration capabilities by expanding the model's scale, which means increasing the number of parameters in the model. This allows not only for the repair of errors or damages within images but also for intelligent restoration based on textual prompts. For example, it can modify specific details in an image according to a given description. This approach improves the quality and intelligence of image restoration, enabling the model to recover and enhance images more accurately and flexibly.

<br/>
<PhotoProvider>
  <PhotoView src="/blog/AI-weekly-005/003.png">
    <img src="/blog/AI-weekly-005/003.png" alt="" />
  </PhotoView>
</PhotoProvider>

* [**Diffuse to Choose:**](https://arxiv.org/abs/2401.13795)

This is an online shopping "virtual try-on" model that allows you to place any product into any environment and perfectly blend it with that environment. For example, you can place a chair from an online store into a photo of your living room to see what it would actually look like there. In essence, it helps users better understand how a product would look in a real-world setting, thereby enhancing the online shopping experience.

<br/>
<PhotoProvider>
  <PhotoView src="/blog/AI-weekly-005/004.png">
    <img src="/blog/AI-weekly-005/004.png" alt="" />
  </PhotoView>
</PhotoProvider>

### üõ†Ô∏è Products you should try

* [**Nijijourney V6:**](https://nijijourney.com/en/)

Niji V6 can now further understand various themes and generate artwork based on those themes. It can even generate themes that are not commonly seen in typical anime. If you want more than just anime style, Niji V6 also offers a "RAW mode" that can create images that look more realistic. If Niji V6 does not understand a concept, users can help it understand by providing explanations.

They plan to introduce a range of new features in the comprehensive release at the end of February, such as vary (to adjust specific parts of an image), pan (to move the image), and zoom (to scale the image), further enhancing user experience and creative flexibility.

<br/>
<PhotoProvider>
  <PhotoView src="/blog/AI-weekly-005/005.png">
    <img src="/blog/AI-weekly-005/005.png" alt="" />
  </PhotoView>
</PhotoProvider>

* [**ElevenLabs AI:**](https://elevenlabs.io/dubbing)

This is a fully automated AI tool. You just need to upload a video or paste a video link, and it can translate your video into 29 languages within a few seconds to a few minutes. Impressively, it can also clone the voices from the original video to provide dubbing. It is capable of translating and cloning the voices of multiple speakers in the video. Besides video files, it can also process audio files, such as MP3, MP4, etc.

<br/>
<PhotoProvider>
  <PhotoView src="/blog/AI-weekly-005/006.png">
    <img src="/blog/AI-weekly-005/006.png" alt="" />
  </PhotoView>
</PhotoProvider>

* [**StreamRAGÔºö**](https://github.com/video-db/StreamRAG)

StreamRAG is a video search and streaming proxy tool that allows you to build a customized personal GPT based on your video data within two minutes. Then, you can have a dialogue with your videos. It also enables you to rapidly browse through a large archive of stored video materials, helping you find and showcase video clips related to the content or themes you are searching for. This way, you can directly watch the parts of the video that are relevant to your search.

<br/>
<PhotoProvider>
  <PhotoView src="/blog/AI-weekly-005/007.png">
    <img src="/blog/AI-weekly-005/007.png" alt="" />
  </PhotoView>
</PhotoProvider>

* [**Lepton Search:**](https://search.lepton.run/)

Lepton Search is an AI search engine created by the Jiayang Qing team using only 500 lines of code. While I usually use the Perplexity engine when researching, this Lepton AI search engine has already achieved results similar to the Perplexity engine. It was built as a demo to demonstrate to developers that there are no difficult AI applications to construct, using just 500 lines of code. It's not a formal product but serves as a showcase of the possibilities in AI application development.

<br/>
<PhotoProvider>
  <PhotoView src="/blog/AI-weekly-005/008.png">
    <img src="/blog/AI-weekly-005/008.png" alt="" />
  </PhotoView>
</PhotoProvider>


<Subscribe />





