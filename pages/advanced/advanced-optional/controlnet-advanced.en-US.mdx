import { Steps, Callout, Tabs } from 'nextra-theme-docs';
import { PhotoProvider, PhotoView } from 'react-image-previewer';
import Subscribe from "components/subscribtion";

# ControlNet Advanced

In the ControlNet basics, I introduced several ways to use ControlNet, but these methods all require you to directly import Pose, Depth, and other graphics into the workflow. What if these pictures aren't available?

The best way is to directly use the existing images and convert them into Pose or Depth images. The advanced chapters will teach you how to use plugins to convert images for ControlNet use.

<Callout type="warning" emoji="âš ï¸">
  Before starting this chapter, please download the following models and place the model files in the corresponding folders:
  * [Dreamshaper](https://civitai.com/models/4384/dreamshaper): Place it within the models/checkpoints folder in ComfyUI.
  * [ControlNet Scribble](https://huggingface.co/lllyasviel/sd-controlnet-scribble/resolve/main/diffusion_pytorch_model.safetensors?download=true): Place it within the models/controlnet folder in ComfyUI.
  * [ControlNet Openpose](https://huggingface.co/lllyasviel/sd-controlnet-openpose/resolve/main/diffusion_pytorch_model.safetensors?download=true): Place it between the models/controlnet folder in ComfyUI.
  * [ControlNet Canny](https://huggingface.co/lllyasviel/ControlNet-v1-1/resolve/main/control_v11p_sd15_canny.pth): Place it between the models/controlnet folder in ComfyUI.
</Callout>

## Image canny ControlNet workflow
In the compulsory course, I introduced the Scribble ControlNet workflow, which can convert hand-drawn drafts into images. However, if you don't have a hand-drawn draft, but want to generate from someone else's picture, I suggest not converting the picture into a scribble picture, but using Canny. The resulting reference picture is more accurate than the hand-drawn style. For example, like this:
<PhotoProvider>
  <PhotoView src="/comfyui-controlnet/011.png">
      <img src="/comfyui-controlnet/011.png" alt="" />
  </PhotoView>
</PhotoProvider>

As you can see from the example above, Canny is somewhat similar to the first Scribble. Canny generates edge maps from existing images, while Scribble involves sketching. Its workflow is also quite simple and you can refer to the Pose ControlNet workflow. All you need to do is replace the DWPose Estimation node with the Canny node.
<Steps>
  ### Download Plugin
  First, you need to download a plugin called [ComfyUI's ControlNet Auxiliary Preprocessors](https://github.com/Fannovel16/comfyui_controlnet_aux?tab=readme-ov-file). There are two ways to install:

  <Tabs items={['ComfyUI-Manager (Recommended)', 'Github']}>
    <Tabs.Tab>
      If you have installed ComfyUI-Manager, you can directly search and install this plugin in ComfyUI-Manager.

      The method to install ComfyUI-Manager, and plug-ins can refer to the tutorial [Install Plugins](../../preparation-for-study/optional/custom-nodes).
    </Tabs.Tab>
    <Tabs.Tab>
      If you don't want to install ComfyUI-Manager, you can directly download this plugin on Github, and then put it in the `custom_nodes` folder in ComfyUI. Then restart ComfyUI.
      <Callout type="warning" emoji="âš ï¸">
        Please note that after restarting ComfyUI, you can double-click on a blank place, and then enter the node names in the plugin. If you can search for it, it means the plugin is installed successfully. If not, try refreshing the browser page, or close the browser page and reopen it, and then search to see if you can find it.
      </Callout>
    </Tabs.Tab>
  </Tabs>
  ### Inserting the Canny Node
  Next, you need to load the previous Canny workflow (or create a new one), then double-click on the blank space, type 'Canny' in the search box, and add that node. Then connect this node's left endpoint with the Load Image node, and the right endpoint with the Apply ControlNet node.
  If you want to get the Canny image, you can also add a Preview Image node to the right endpoint of the Canny node, so you can see the Canny image. The final workflow is as shown in the picture:
  <br/>
  <PhotoProvider>
    <PhotoView src="/comfyui-controlnet/012.png">
        <img src="/comfyui-controlnet/012.png" alt="" />
    </PhotoView>
  </PhotoProvider>
</Steps>

## Image pose ControlNet workflow

But if you can't directly draw the pose, you can try importing a picture you think is appropriate, and then convert it into a pose through the plugin, and then input it into the ControlNet model. This way you can control the pose of the character generated by the model through the image.

The method is also simple.
<Steps>
  ### Insert DWPose Estimation Node
  Double-click on a blank place, enter DWPose Estimation in the search box, and then add this node. Then connect this node's left endpoint with the Load Image node. Parameter configuration is as follows:
  * detect_hand: disable
  * detect_body: enable
  * detect_face: disable
  * resolution: 512 (because the image we generate is 512 * 512 so here is set to 512 * 512)
  * bbox_detector: yolox_l.onnx
  * pose_estimator: dw-ll_ucoco_384.onnx

  The first three, because we mainly want to control the pose of the character, so here we only need to detect the body of the character. The last two settings can be faster.
  ### Add Upscale Image Node(Optional)
  The addition of this step is mainly to make the resolution of the picture consistent with the resolution of the generated picture, so as to better control the picture generated by the model. You can use this technique with other ControlNet.
  
  We can also add an Upscale Image node, and then connect the right endpoint of DWPose Estimation with the left endpoint of Upscale Image. The parameter configuration is as follows:
  * upscale_method: nearest-exact
  * width: 512
  * height: 512
  * crop: disable
  
  Because the arrangement of the pixels in the pose image is quite regular, we choose nearest-exact, and then set the width and height to match the generated image, and finally disable crop.

  After adding this node, we can connect the right endpoint of Upscale Image with the left endpoint of Apply ControlNet.

  Also, if you want to get the pose image, you can add a Preview Image node, and then connect it with the right endpoint of DWPose Estimation, so you can see the pose image.
  <br/>
  <PhotoProvider>
    <PhotoView src="/comfyui-controlnet/008.png">
        <img src="/comfyui-controlnet/008.png" alt="" />
    </PhotoView>
  </PhotoProvider>

</Steps>

## Image depth ControlNet workflow
ðŸš§ Under construction

## Reference-only ControlNet workflow
Lastly, you may encounter a situation where your client provides reference images for your design, for example design a logo. In this case, besides letting the AI generate directly, you can also use these reference images to let AI produce a new logo.

<Callout emoji="ðŸ’¡">
  f you're interested in how to use AI to generate logos, feel free to read our blog article: [How to generate App logo using ComfyUI?](/blog/generate-app-logo)
</Callout>
<Steps>

### Download the extension
In order to build this workflow, you'll need to use the experimental nodes of ComfyUI, so you need to first install the [ComfyUI_experiments](https://github.com/comfyanonymous/ComfyUI_experiments) plugin. You can use the ComfyUI Manager to install, or manually download and import it for installation.

### Insert the ReferenceOnlySimple Node
All you have to do is replace the Empty Latent Image in the original ControlNet workflow with a reference image. First, double-click anywhere blank, search 'Reference' and you'll find the ReferenceOnlySimple node, then add that node and connect it to the Ksampler. Because this node only supports importing latents, you'll need to use VAE Encode to convert the image into latents. The final workflow is as shown in the picture below:

<br/>
  <PhotoProvider>
    <PhotoView src="/blog/generate-app-logo/007.png">
      <img src="/blog/generate-app-logo/007.png" alt="" />
    </PhotoView>
  </PhotoProvider>
</Steps>

<Subscribe />