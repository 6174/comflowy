---
title: ContrlNet
description: Master the use of ControlNet in Stable Diffusion with this comprehensive guide. Learn how to control the construction of the graph for better results in AI image generation. Understand the principles of ControlNet and follow along with practical examples, including how to use sketches to control image output.
keywords: [ControlNet, Stable Diffusion, Graph Construction, AI Image Generation, ControlNet Principles, ControlNet Examples, Sketch Control, Image Output, ControlNet Workflow, AI Tools, Image Manipulation, AI Workflow, AI Tutorial]
---

import { Steps, Callout, Tabs } from 'nextra-theme-docs';
import { PhotoProvider, PhotoView } from 'react-image-previewer';
import Subscribe from "components/subscribtion";

# ContrlNet
When using Stable Diffusion, you may wish to control the construction of the graph, but adjusting via prompts may not yield great results. This chapter will teach you several common methods of using ControlNet to control the graph construction.

<Callout type="warning" emoji="âš ï¸">
  Before starting this chapter, please download the following models and place the model files in the corresponding folders:
  * [Dreamshaper](https://civitai.com/models/4384/dreamshaper): Place it within the models/checkpoints folder in ComfyUI.
  * [ControlNet Scribble](https://huggingface.co/lllyasviel/sd-controlnet-scribble/resolve/main/diffusion_pytorch_model.safetensors?download=true): Place it within the models/controlnet folder in ComfyUI.
  * [ControlNet Openpose](https://huggingface.co/lllyasviel/sd-controlnet-openpose/resolve/main/diffusion_pytorch_model.safetensors?download=true): Place it between the models/controlnet folder in ComfyUI.
</Callout>

## Principle Introduction
Essentially, all the methods I teach in the advanced tutorial are image-to-image methods. They all provide different information to the model through images, so the model can generate the images we want. ControlNet controls the images that the model generates based on the structural information of the image. This structural information could be a sketch, a mask of an image, or even the edge information of an image. All these information can be used to control the generation of images by the model through ControlNet.
You can choose different ControlNet to control the image generated by the model according to your needs.

As usual, to help you better understand how to use ControlNet, we will first introduce the principle of ControlNet visually:
<br/>
<PhotoProvider>
  <PhotoView src="/comfyui-controlnet/003.png">
      <img src="/comfyui-controlnet/003.png" alt="" width="90%" style={{ display: "block", margin: "0 auto" }}/>
  </PhotoView>
</PhotoProvider>

From the above picture, we can see that when we use ControlNet, we first input the text prompt and image into the ControlNet model. Then, the ControlNet model generates a latent image. The latent image will be used as Conditioning and the initial prompt to input into the Stable Diffusion model, thus affecting the image generated by the model.


## Scribble ControlNet Workflow

Through the introduction of the principle, you should be able to deduce how to use ControlNet in ComfyUI. We might as well try how to build a simple ControlNet workflow - control with a simple sketch. The effect is roughly as follows:
<br/>
<PhotoProvider>
  <PhotoView src="/comfyui-controlnet/001.png">
      <img src="/comfyui-controlnet/001.png" alt="" width="80%" style={{ display: "block", margin: "0 auto" }}/>
  </PhotoView>
</PhotoProvider>

With ControlNet, the image output of the model will construct the image according to the sketches you draw. As you can see from the left sketch, it's a very rough cartoon character, while the generated image on the right matches the basic composition and character features (two big ears) of the sketch on the left.

OK, now that you have an impression of Scribble ControlNet, let's see how to construct this workflow together. You can try to construct it on your own first. This way, it can deepen your impression.

<Tabs items={['Hint', 'Solution']}>
  <Tabs.Tab>
    <Callout emoji="ðŸ’¡">
      In the LoRA chapter, I compared LoRA to a filter. For ControlNet, I believe it's more like a visual prompt supplement, which visualizes the prompts that are hard to describe with text, and also helps the model understand better. It thus solves the problem that the CLIP model's understanding of grammar is quite poor. If you understand it this way, you should be better able to reason and remember how to connect the lines. LoRA is a filter, affecting the model, so it's connected to the Model. And ControlNet is a supplement to the prompt, controlling Conditioning, so it's connected to the Prompt node.
    </Callout>
  </Tabs.Tab>
  <Tabs.Tab>
    <Steps>
      ### Load Default Workflow and Add Load Image Node
      Let's start with the Default workflow (you can load the default workflow by clicking the Load Default button on the right), and then think about how to modify it.

      First, recall the principle of ControlNet, we need to enter the text prompt and image into the ControlNet model together, so our first step is to add a Load Image node.
      <br/>
      <PhotoProvider>
        <PhotoView src="/comfyui-controlnet/004.png">
            <img src="/comfyui-controlnet/004.png" alt="" width="80%" style={{ display: "block", margin: "0 auto" }}/>
        </PhotoView>
      </PhotoProvider>

      ### Add Apply ControlNet Node
      As shown in the figure above, the second step should be to run the ControlNet model, so we need to add an Apply ControlNet node.

      Click on a blank space with your mouse, enter Apply ControlNet, and add this node. The strength in the node represents the influence strength of ControlNet, the larger the value, the more the image generated in the end will resemble the input structural diagram. This value is greatly related to the model you use, you can test it several times according to your own needs, I usually set it to 0.9.

      ### Connect Text Encode Node and Add Load ControlNet Model Node
      For the third step, we connect the end point of Positive Text Encode with the conditioning end point of Apply ControlNet.

      Also, if you look closely at the Apply ControlNet node, there is a control_net end point on the left, which means it needs to connect to the ControlNet model, so double-click on a blank space, search and add the Load ControlNet Model node, and then select the ControlNet Scribble model.

      ### Connect KSampler Node
      Finally, connect the conditioning end point of Apply ControlNet with the positive end point of KSampler, and this completes the construction of ControlNet.

      Also note here, in the schematic, ControlNet's output latent image and Positive Text Encode are actually input into the Stable Diffusion model together. Logically, there should be two conditionings (one for Text and one for images), but since in ComfyUI, an input side can only be connected to one output side, you can't connect to two output sides, so we only connected one line in ComfyUI.

      ### Adjust Parameters & Model
      Finally, adjust the parameters & model, the parameters of KSampler can be similar to the normal text life diagram, I usually use Dpmpp 2m and Karras, the number of steps and cfg can be configured according to your needs. There's also one point that needs attention, the model in Checkpoint needs to be based on SD 1.5, such as Anything I used in the case or directly using SD 1.5.

      If you didn't use the ControlNet Scribble I recommended in Load ControlNet but other Scribble models, you need to switch to the corresponding Checkpoint model of this ControlNet model.
      <br/>
      <PhotoProvider>
        <PhotoView src="/comfyui-controlnet/002.png">
            <img src="/comfyui-controlnet/002.png" alt="" />
        </PhotoView>
      </PhotoProvider>
      <Callout emoji="ðŸ’¡">
        You might wonder why I'm explaining it in such detail? To use ControllNet, isn't it just enough to load a workflow directly? Indeed, if you just want to use ControlNet to generate images, directly importing workflow is the most efficient way. But what if you need to use multiple models at the same time? For example, adding LoRA on the basis of ControlNet, then you need to build your own workflow. So I still suggest that you learn how to build ControlNet's workflow step by step.
      </Callout>
    </Steps>
  </Tabs.Tab>
</Tabs>

## Pose ControlNet Workflow

Once you can build a ControlNet workflow, you can freely switch between different models according to your needs.

The previous example used a sketch as an input, this time we try inputting a character's pose. The advantage of this is that you can use it to control the pose of the character generated by the model. Like this:

<PhotoProvider>
  <PhotoView src="/comfyui-controlnet/005.jpg">
      <img src="/comfyui-controlnet/005.jpg" alt="" width="80%" style={{ display: "block", margin: "0 auto" }}/>
  </PhotoView>
</PhotoProvider>

However, note that we can't directly input the image into the ControlNet model like in the previous example, but need to first convert the image into a pose, and then input it into the ControlNet model. But you can also use other tools to make a skeleton diagram, and then directly input it into the ControlNet model.
<br/>
<PhotoProvider>
  <PhotoView src="/comfyui-controlnet/006.jpg">
      <img src="/comfyui-controlnet/006.jpg" alt="" />
  </PhotoView>
</PhotoProvider>
So the construction of the entire workflow is the same as the previous workflow, only in the Load ControlNet Model node, we need to load the ControlNet Openpose model, and load the skeleton diagram:
<br/>
<PhotoProvider>
  <PhotoView src="/comfyui-controlnet/007.png">
      <img src="/comfyui-controlnet/007.png" alt="" />
  </PhotoView>
</PhotoProvider>

But if you can't directly draw the pose, you can try importing a picture you think is appropriate, and then convert it into a pose through the plugin, and then input it into the ControlNet model. This way you can control the pose of the character generated by the model through the image.

The method is also simple.
<Steps>
  ### Download Plugin
  First, you need to download a plugin called [ComfyUI's ControlNet Auxiliary Preprocessors](https://github.com/Fannovel16/comfyui_controlnet_aux?tab=readme-ov-file). There are two ways to install:

  <Tabs items={['ComfyUI-Manager (Recommended)', 'Github']}>
    <Tabs.Tab>
      If you have installed ComfyUI-Manager, you can directly search and install this plugin in ComfyUI-Manager.

      The method to install ComfyUI-Manager, and plug-ins can refer to the tutorial [Install Plugins](../../preparation-for-study/optional/custom-nodes).
    </Tabs.Tab>
    <Tabs.Tab>
      If you don't want to install ComfyUI-Manager, you can directly download this plugin on Github, and then put it in the `custom_nodes` folder in ComfyUI. Then restart ComfyUI.
      <Callout type="warning" emoji="âš ï¸">
        Please note that after restarting ComfyUI, you can double-click on a blank place, and then enter the node names in the plugin. If you can search for it, it means the plugin is installed successfully. If not, try refreshing the browser page, or close the browser page and reopen it, and then search to see if you can find it.
      </Callout>
    </Tabs.Tab>
  </Tabs>
  ### Insert DWPose Estimation Node
  Double-click on a blank place, enter DWPose Estimation in the search box, and then add this node. Then connect this node's left endpoint with the Load Image node. Parameter configuration is as follows:
  * detect_hand: disable
  * detect_body: enable
  * detect_face: disable
  * resolution: 512 (because the image we generate is 512 * 512 so here is set to 512 * 512)
  * bbox_detector: yolox_l.onnx
  * pose_estimator: dw-ll_ucoco_384.onnx

  The first three, because we mainly want to control the pose of the character, so here we only need to detect the body of the character. The last two settings can be faster.
  ### Add Upscale Image Node
  The addition of this step is mainly to make the resolution of the picture consistent with the resolution of the generated picture, so as to better control the picture generated by the model. We can also add an Upscale Image node, and then connect the right endpoint of DWPose Estimation with the left endpoint of Upscale Image. The parameter configuration is as follows:
  * upscale_method: nearest-exact
  * width: 512
  * height: 512
  * crop: disable
  
  Because the arrangement of the pixels in the pose image is quite regular, we choose nearest-exact, and then set the width and height to match the generated image, and finally disable crop.

  After adding this node, we can connect the right endpoint of Upscale Image with the left endpoint of Apply ControlNet.

  Also, if you want to get the pose image, you can add a Preview Image node, and then connect it with the right endpoint of DWPose Estimation, so you can see the pose image.
  <br/>
  <PhotoProvider>
    <PhotoView src="/comfyui-controlnet/008.png">
        <img src="/comfyui-controlnet/008.png" alt="" />
    </PhotoView>
  </PhotoProvider>

</Steps>

## Depth ControlNet Workflow

The third use of ControlNet is to control the generated images through depth maps. The advantage of this method is that you can control the depth of field of the generated images through depth maps. For example, like this:

This workflow is also similar to the previous one. You can directly import the depth map, or use a plugin to generate the depth map, and then input it into the ControlNet model. Like this:
<br/>
<PhotoProvider>
  <PhotoView src="/comfyui-controlnet/009.png">
      <img src="/comfyui-controlnet/009.png" alt="" />
  </PhotoView>
</PhotoProvider>

This has more depth of field information than pose. For example, my imported image is of two people fighting, standing one in front of the other. If only using pose to draw, it's relatively difficult to draw this kind of front-back image.

If you want to generate depth maps using plugins like in the pose workflow, the method is also straightforward. Simply replace the DWPose Estimation node in the above workflow with the Zoe-Depth Map node.

Moreover, there's another method. You can also use some 3D tools to generate character poses or depth maps. For example, [Posemy.art](https://app.posemy.art/) is such a product. You can choose the character's pose you want in the upper left corner (marked 1), then adjust the character's pose by dragging the mouse, and finally click the Export button (marked 2) to export the depth map (marked 3), and then import it into ComfyUI:
<br/>
<PhotoProvider>
  <PhotoView src="/comfyui-controlnet/010.png">
      <img src="/comfyui-controlnet/010.png" alt="" />
  </PhotoView>
</PhotoProvider>
<Subscribe />